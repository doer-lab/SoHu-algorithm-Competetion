{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文件地址管理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_info_train_path = './News_info_train_filter.txt'\n",
    "pic_info_train_path = ''\n",
    "news_pic_label_train_path = './News_pic_label_train.txt'\n",
    "\n",
    "news_info_unlabel_path = './News_info_unlabel_filter.txt'\n",
    "pic_info_unlabel_path = ''\n",
    "\n",
    "news_info_validate_path = './News_info_validate_filter.txt'\n",
    "pic_info_validate_path = ''\n",
    "news_pic_label_validate_path = ''\n",
    "\n",
    "ltp_model_path = './NLP/ltp_data_v3.4.0/cws.model'   # Ltp 3.4 分词模型库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取原始数据（返回的数据是一个列表,其中每一条数据包括id, content, picture list or text）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from instrument import read_text\n",
    "\n",
    "news_info_train = read_text(news_info_train_path)                     # 已标注训练集\n",
    "news_pic_label_train = read_text(news_pic_label_train_path)           # 已标注训练集对应的标签\n",
    "news_info_validate = read_text(news_info_validate_path)               # 未标注验证集（利用训练好的模型进行标注）\n",
    "news_info_unlabel = read_text(news_info_unlabel_path)                 # 未标注数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取每条新闻文本的id, content, pic_list, label, match_pic_list, match_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、利用data2bunch函数对训练集、验证集、无标签数据的文本数据进行提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from instrument import data2bunch\n",
    "\n",
    "train_bunch = data2bunch(news_info_train, label=False)          # 对训练集数据进行提取\n",
    "validate_bunch = data2bunch(news_info_validate, label=False)    # 对验证集数据进行提取\n",
    "train_unlabel_bunch = data2bunch(news_info_unlabel,label=False) # 对无标签数据集进行数据提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、利用data2bunch函数对训练集中文本和图片的标签进行提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = data2bunch(news_pic_label_train,label=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、进行文本与标签的配对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判断判断文本的id与标签的id是否相同来判断文本与标签是否匹配\n",
    "for i in range(len(train_bunch.news_id)):\n",
    "    if train_bunch.news_id[i] != train_label.news_pic_id[i]:\n",
    "        print('出现标签不匹配！')\n",
    "        print(train_bunch.news_id[i],'is not equal to',train_label.news_pic_id[i])\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中文分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 有待改进：1、选择保留哪些标点符号的方法不是很合理；2、某种符号可能出现一串的存在的情况，只要要保存一个即可；3、未取出停止词便统计句子的长度；4、ltp 分词方法中可解决先去除停止词再计算句子长度（先分词 -> 不带停止词的词列表 -> 计算句长 -> ' '.join(…)）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2words_v2(need_to_segment_text, method='jieba', ltp_model_path='',postdict={},stop_words=[]):\n",
    "    '''\n",
    "        该函数实现对每条新闻文本进行分词（分词方法包括jieba和ltp）,并将繁体字替换为简体字\n",
    "    1、need_to_segment_text 为需要进行分词的文本列表\n",
    "    2、method 为中文分词方法，可选 jieba 或 ltp\n",
    "    3、ltp_model_path 为选择ltp分词方法时，ltp 分词模型在本地的目录地址\n",
    "    4、postdist 数据格式：\n",
    "        postdict = {'解 空间':'解空间','深度 优先':'深度优先'}\n",
    "    5、计算出每条句子中所包含的字、词和某些符号的总数，作为该条句子的总长度，最终返回句子长度的列表\n",
    "    \n",
    "    '''\n",
    "    from langconv import Converter\n",
    "    import re\n",
    "    text_words = []\n",
    "    sentence_len = []            # 记录句子的长度（按分词后句子所含字或词的个数计算）\n",
    "\n",
    "    if method == 'jieba':\n",
    "        import jieba\n",
    "#         jieba.enable_parallel(4)             # 多线程分词，仅支持 Linux\n",
    "        for sentence in need_to_segment_text:\n",
    "#             print(sentence)                       # 查看新闻原文\n",
    "            sentence = Converter('zh-hans').convert(sentence)             # 将繁体中文转换为简体中文\n",
    "#             content = re.sub('[^\\u4e00-\\u9fa5a-zA-Z0-9.]',' ', sentence)     # 将中文、大小写字母和数字外的字符全替换为空格\n",
    "            content = re.sub(\"[^\\u4e00-\\u9fa5a-zA-Z0-9|，。？！、；：“”‘’（）【】{}……《》%,.?!;:'()[]-——/&]\",' ', sentence)\n",
    "            content = re.sub('\\s+',' ',content)\n",
    "            content = re.sub('//\\s+(//)?','/',content)\n",
    "            word_list = [word for word in jieba.cut(content) if word not in stop_words]  # 分词，去停止词\n",
    "            sentence_len.append(len(word_list))             # 记录该句话的长度\n",
    "            seg_sent = ' '.join(word_list)  # 去除停止词\n",
    "            seg_sent = re.sub('\\s+',' ',seg_sent)\n",
    "            for key in postdict:\n",
    "                seg_sent = seg_sent.replace(key,postdict[key])    # 在分词后处理某些被分错的词和词语\n",
    "            text_words.append(seg_sent)\n",
    "    elif method == 'ltp' and ltp_model_path != '':\n",
    "        from pyltp import Segmentor\n",
    "        #         model_path = 'E:/Desktop/ZhuFei/Competition/NLP/ltp_data_v3.4.0/cws.model'   # Ltp 3.4 分词模型库\n",
    "        segmentor = Segmentor()   # 实例化分词模块\n",
    "        segmentor.load(ltp_model_path)  # 加载分词库\n",
    "        for sentence in need_to_segment_text:\n",
    "            #             print(sentence)                         # 查看新闻原文\n",
    "            sentence = Converter('zh-hans').convert(sentence)     # 将繁体中文转换为简体中文\n",
    "            content = re.sub(\"[^\\u4e00-\\u9fa5a-zA-Z0-9|，。？！、；：“”‘’（）【】{}……《》%,.?!;:'()[]-——/&]\",' ', sentence)\n",
    "            content = re.sub('\\s+','',content)\n",
    "            content = re.sub('//\\s+(//)?','/',content)\n",
    "            word_list = [word for word in segmentor.cut(content) if word not in stop_words]\n",
    "            sentence_len.append(len(word_list))                       # 记录该句话的长度\n",
    "            seg_sent = ' '.join(word_list)  # 去除停止词\n",
    "            seg_sent = re.sub('\\s+',' ',seg_sent)\n",
    "            for key in postdict:\n",
    "                seg_sent = seg_sent.replace(key,postdict[key])    # 在分词后处理某些被分错的词和词语\n",
    "            text_words.append(seg_sent) # 去除停止词\n",
    "    else:\n",
    "        fill_to_length = 130\n",
    "        print(''.center(fill_to_length, '#'))\n",
    "        print(' Method or model path is wrong! Please check it!!!! '.center(fill_to_length, '#'))\n",
    "        print(''.center(fill_to_length, '#'))\n",
    "\n",
    "    return [text_words,sentence_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "postdict = {'基姆 霍根':'基姆•霍根',\n",
    "            '中央 商场':'中央商场',\n",
    "            '马克 龙':'马克龙',\n",
    "            ' 惊天 魔 盗团':' 惊天魔盗团',\n",
    "            '天将 雄狮':'天降雄狮',\n",
    "            '天降 雄狮':'天降雄狮',\n",
    "            '盗墓 笔记':'盗墓笔记',\n",
    "            '功夫 瑜伽':'功夫瑜伽',\n",
    "            '大闹 天竺':'大闹天竺',\n",
    "            '人力 资源管理 师':'人力资源管理师',\n",
    "            '微信 公众 号':'微信公众号',\n",
    "            '星球大战 之 最后 的 绝地 武士':'星球大战之最后的绝地武士',\n",
    "            '前任 3   再见 前任':'前任3:再见 前任',\n",
    "            '二代 妖精':'二代妖精',\n",
    "            '妖猫 传':' 妖猫传',\n",
    "            '解忧 杂货店':'解忧杂货店',\n",
    "            '机器 之血':'机器之血',\n",
    "            '寻梦 环 游记':'寻梦环游记'\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、对训练集中的新闻文本进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_content_words_jieba, train_content_sentence_length = text2words_v2(train_bunch.news_content)    # jieba 分词\n",
    "\n",
    "# model_path = './NLP/ltp_data_v3.4.0/cws.model'   # Ltp 3.4 分词模型库\n",
    "# train_content_words_ltp = text2words(train_bunch.news_content,method='ltp',model_path=model_path) # ltp 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_content_words_jieba)\n",
    "print(train_content_sentence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、对验证集中的新闻文本进行分词 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_content_words_jieba, validate_content_sentence_length = text2words_v2(validate_bunch.news_content)   # jieba 分词 \n",
    "\n",
    "# validate_content_words_ltp = text2words(validate_bunch.news_content,method='ltp',model_path=model_path) # ltp 分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、对未标注文本进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabel_news_words_jieba, unlabel_news_sentence_length = text2words_v2(train_unlabel_bunch.news_content)   # jieba 分词 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、训练集的标签匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_bunch.news_id)):\n",
    "    if train_bunch.news_id[i]!=train_label.news_pic_id[i]:\n",
    "        print(train_bunch.news_id[i],'is not equal with',train_label.news_pic_id[i])\n",
    "        break\n",
    "    else:\n",
    "        pass\n",
    "#         print('ID is correct!')\n",
    "dir(train_bunch)\n",
    "dir(train_label)\n",
    "print(dir(validate_bunch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 到此为止，上述代码完成了如下任务：将文本数据读入后按行保存为列表(中文文本数据和标签文本数据) -> 将中文文本数据进行分词保存为列表（按特征对数据进行分离；标点符号保留、繁体替换、多余符号仅保留一个 ）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5、对标签进行独热编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_one_hot = np.array(pd.get_dummies(train_label.news_pic_label)).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5、将分词结果保存以方便后续使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1、将训练集与验证集进行Bunch化 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.base import Bunch\n",
    "\n",
    "# 1、将训练集的分词结果保存为本地的一个 Bunch 对象\n",
    "TRAIN_BUNCH = Bunch(news_id=[],news_words_jieba=[],news_length=[],news_pic_list=[])\n",
    "TRAIN_BUNCH.news_id = train_bunch.news_id\n",
    "TRAIN_BUNCH.news_words_jieba = train_content_words_jieba\n",
    "TRAIN_BUNCH.news_length = train_content_sentence_length\n",
    "TRAIN_BUNCH.news_pic_list = train_bunch.news_pic_list\n",
    "\n",
    "# 2、将训练集新闻和图片对应的标签保存为本地的一个 Bunch 对象\n",
    "TRAIN_LABEL_BUNCH = Bunch(news_pic_id=[],news_pic_label=[],news_pic_label_one_hot=[],news_pic_pic=[],news_pic_text=[])\n",
    "TRAIN_LABEL_BUNCH.news_pic_id = train_label.news_pic_id\n",
    "TRAIN_LABEL_BUNCH.news_pic_label = train_label.news_pic_label\n",
    "TRAIN_LABEL_BUNCH.news_pic_label_one_hot = label_one_hot\n",
    "TRAIN_LABEL_BUNCH.news_pic_pic = train_label.news_pic_pic\n",
    "TRAIN_LABEL_BUNCH.news_pic_text = train_label.news_pic_text\n",
    "\n",
    "# 3、将未标注数据集的分词结果保存为本地的一个 Bunch 对象\n",
    "UNLABEL_NEWS_BUNCH = Bunch(news_id=[],news_words_jieba=[],news_length=[],news_pic_list=[])\n",
    "UNLABEL_NEWS_BUNCH.news_id = train_unlabel_bunch.news_id\n",
    "UNLABEL_NEWS_BUNCH.news_words_jieba = unlabel_news_words_jieba\n",
    "UNLABEL_NEWS_BUNCH.news_length = unlabel_news_sentence_length\n",
    "UNLABEL_NEWS_BUNCH.news_pic_list = train_unlabel_bunch.news_pic_list\n",
    "\n",
    "# 4、将验证集（contain id, content and pictures list）保存为一个本地的 Bunch 对象\n",
    "VALIDATE_BUNCH = Bunch(news_id=[],news_words_jieba=[],news_length=[],news_pic_list=[])\n",
    "VALIDATE_BUNCH.news_id = validate_bunch.news_id\n",
    "VALIDATE_BUNCH.news_words_jieba = validate_content_words_jieba\n",
    "VALIDATE_BUNCH.news_length = validate_content_sentence_length\n",
    "VALIDATE_BUNCH.news_pic_list = validate_bunch.news_pic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# train_df = pd.DataFrame.from_dict(VALIDATE_BUNCH).set_index('news_id')\n",
    "# # train_df = pd.DataFrame.from_dict(TRAIN_LABEL_BUNCH).set_index('news_pic_id')\n",
    "# train_df[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2、将 Bunch 对象保存至本地指定的目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "from instrument import save_bunch\n",
    "\n",
    "train_bunch_path = './data_bunch/cnn_non_stop_words_train_bunch.dat'\n",
    "save_bunch(train_bunch_path,TRAIN_BUNCH)\n",
    "\n",
    "train_label_bunch_path = './data_bunch/cnn_train_label_bunch.dat'\n",
    "save_bunch(train_label_bunch_path,TRAIN_LABEL_BUNCH)\n",
    "\n",
    "unlabel_news_bunch_path = './data_bunch/cnn_non_stop_words_unlabel_news_bunch.dat'\n",
    "save_bunch(unlabel_news_bunch_path,UNLABEL_NEWS_BUNCH)\n",
    "\n",
    "validate_bunch_path = './data_bunch/cnn_non_stop_words_validate_bunch.dat'\n",
    "save_bunch(validate_bunch_path,VALIDATE_BUNCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextCnn模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1、前向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### embedding layer -> convolutional layer -> max pooling layer -> full connecton layer -> full connecton layer -> softmax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FLAGS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-224-8497b8dad735>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0minstrument\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mread_bunch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_bunch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_data_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'FLAGS' is not defined"
     ]
    }
   ],
   "source": [
    "from instrument import read_bunch\n",
    "\n",
    "train_data = read_bunch(FLAGS.training_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48480"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_content_words_jieba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_length = 22502\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 22502)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(vocab_processor.fit_transform(train_content_words_jieba[:10]))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
