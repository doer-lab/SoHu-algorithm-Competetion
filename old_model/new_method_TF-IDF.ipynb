{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文件地址管理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 后期考虑先设定工作目录，再读取目录中的数据文件\n",
    "news_info_train_path = './News_info_train_filter.txt'\n",
    "pic_info_train_path = ''\n",
    "news_pic_label_train_path = './News_pic_label_train.txt'\n",
    "\n",
    "news_info_unlabel_path = './News_info_unlabel_filter.txt'\n",
    "pic_info_unlabel_path = ''\n",
    "\n",
    "news_info_validate_path = './News_info_validate_filter.txt'\n",
    "pic_info_validate_path = ''\n",
    "news_pic_lavel_validate_path = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取原始的文本数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from instrument import read_text\n",
    "\n",
    "news_info_train = read_text(news_info_train_path)\n",
    "news_pic_label_train = read_text(news_pic_label_train_path)\n",
    "news_info_validate = read_text(news_info_validate_path)\n",
    "news_info_unlabel = read_text(news_info_unlabel_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取每条新闻文本的id, content, pic_list, label, match_pic_list, match_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、利用data2bunch函数对训练集、验证集、无标签数据的文本数据进行提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from instrument import data2bunch\n",
    "\n",
    "train_bunch = data2bunch(news_info_train, label=False)          # 对训练集数据进行提取\n",
    "validate_bunch = data2bunch(news_info_validate, label=False)    # 对验证集数据进行提取\n",
    "train_unlabel_bunch = data2bunch(news_info_unlabel,label=False) # 对无标签数据集进行数据提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、利用data2bunch函数对训练集中文本和图片的标签进行提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = data2bunch(news_pic_label_train,label=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、进行文本与标签的配对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判断判断文本的id与标签的id是否相同来判断文本与标签是否匹配\n",
    "for i in range(len(train_bunch.news_id)):\n",
    "    if train_bunch.news_id[i] != train_label.news_pic_id[i]:\n",
    "        print('出现标签不匹配！')\n",
    "        print(train_bunch.news_id[i],'is not equal to',train_label.news_pic_id[i])\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中文分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " 'data2bunch',\n",
       " 'perdict_bayes',\n",
       " 'read_bunch',\n",
       " 'read_image',\n",
       " 'read_text',\n",
       " 'save_bunch',\n",
       " 'save_image',\n",
       " 'save_text',\n",
       " 'search_best_para_bayes',\n",
       " 'text2words']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import instrument\n",
    "dir(instrument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "postdict = {'基姆 霍根':'基姆•霍根',\n",
    "            '中央 商场':'中央商场',\n",
    "            '马克 龙':'马克龙',\n",
    "            ' 惊天 魔 盗团':' 惊天魔盗团',\n",
    "            '天将 雄狮':'天降雄狮',\n",
    "            '天降 雄狮':'天降雄狮',\n",
    "            '盗墓 笔记':'盗墓笔记',\n",
    "            '功夫 瑜伽':'功夫瑜伽',\n",
    "            '大闹 天竺':'大闹天竺',\n",
    "            '人力 资源管理 师':'人力资源管理师',\n",
    "            '微信 公众 号':'微信公众号',\n",
    "            '星球大战 之 最后 的 绝地 武士':'星球大战之最后的绝地武士',\n",
    "            '前任 3   再见 前任':'前任3:再见 前任',\n",
    "            '二代 妖精':'二代妖精',\n",
    "            '妖猫 传':' 妖猫传',\n",
    "            '解忧 杂货店':'解忧杂货店',\n",
    "            '机器 之血':'机器之血',\n",
    "            '寻梦 环 游记':'寻梦环游记'\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2words(need_to_segment_text, method='jieba', ltp_model_path='',postdict={},stop_words=[]):\n",
    "    '''\n",
    "        该函数实现对每条新闻文本进行分词（分词方法包括jieba和ltp）,并将繁体字替换为简体字\n",
    "    1、need_to_segment_text 为需要进行分词的文本列表\n",
    "    2、method 为中文分词方法，可选 jieba 或 ltp\n",
    "    3、ltp_model_path 为选择ltp分词方法时，ltp 分词模型在本地的目录地址\n",
    "    4、postdist 数据格式：\n",
    "        postdict = {'解 空间':'解空间','深度 优先':'深度优先'}\n",
    "    \n",
    "    '''\n",
    "    from langconv import Converter\n",
    "    import re\n",
    "    text_words = []\n",
    "\n",
    "    if method == 'jieba':\n",
    "        import jieba\n",
    "#         jieba.enable_parallel(4)             # 多线程分词，仅支持 Linux\n",
    "        for sentence in need_to_segment_text:\n",
    "            #             print(sentence)                                # 查看新闻原文\n",
    "            sentence = Converter('zh-hans').convert(sentence)            # 将繁体中文转换为简体中文\n",
    "            content = re.sub('[^\\u4e00-\\u9fa5a-zA-Z0-9.]',' ', sentence)  # 将中文、大小写字母和数字外的字符全替换为空格\n",
    "            content_word = jieba.cut(content)\n",
    "            seg_sent = ' '.join([word for word in content_word if word not in stop_words])  # 去除停止词\n",
    "            seg_sent = re.sub('\\s+',' ',seg_sent)\n",
    "            for key in postdict:\n",
    "                seg_sent = seg_sent.replace(key,postdict[key])    # 在分词后处理某些被分错的词和词语\n",
    "            text_words.append(seg_sent)\n",
    "    elif method == 'ltp' and ltp_model_path != '':\n",
    "        from pyltp import Segmentor\n",
    "        #         model_path = 'E:/Desktop/ZhuFei/Competition/NLP/ltp_data_v3.4.0/cws.model'   # Ltp 3.4 分词模型库\n",
    "        segmentor = Segmentor()   # 实例化分词模块\n",
    "        segmentor.load(ltp_model_path)  # 加载分词库\n",
    "        for sentence in need_to_segment_text:\n",
    "            #             print(sentence)                         # 查看新闻原文\n",
    "            sentence = Converter('zh-hans').convert(sentence)     # 将繁体中文转换为简体中文\n",
    "            content = re.sub('[^\\u4e00-\\u9fa5a-zA-Z0-9.:]',' ', sentence)  # 将中文、大小写字母和数字外的字符全替换为空格\n",
    "            content_word = jieba.cut(content)\n",
    "            seg_sent = ' '.join([word for word in content_word if word not in stop_words])  # 去除停止词\n",
    "            seg_sent = re.sub('\\s+',' ',seg_sent)\n",
    "            for key in postdict:\n",
    "                seg_sent = seg_sent.replace(key,postdict[key])    # 在分词后处理某些被分错的词和词语\n",
    "            text_words.append(seg_sent) # 去除停止词\n",
    "    else:\n",
    "        fill_to_length = 130\n",
    "        print(''.center(fill_to_length, '#'))\n",
    "        print(' Method or model path is wrong! Please check it!!!! '.center(fill_to_length, '#'))\n",
    "        print(''.center(fill_to_length, '#'))\n",
    "\n",
    "    return text_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from instrument import text2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# text2words(train_bunch.news_content[69:79],postdict=postdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、对训练集中的新闻文本进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_content_words_jieba = text2words(train_bunch.news_content,postdict=postdict)    # jieba 分词\n",
    "\n",
    "# model_path = './NLP/ltp_data_v3.4.0/cws.model'   # Ltp 3.4 分词模型库\n",
    "# train_content_words_ltp = text2words(train_bunch.news_content,method='ltp',model_path=model_path) # ltp 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48480"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_content_words_jieba)\n",
    "# train_content_words_jieba[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、对验证集中的新闻文本进行分词 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_content_words_jieba = text2words(validate_bunch.news_content,postdict=postdict)   # jieba 分词 \n",
    "\n",
    "# validate_content_words_ltp = text2words(validate_bunch.news_content,method='ltp',model_path=model_path) # ltp 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9696"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validate_content_words_jieba)\n",
    "# validate_content_words_jieba[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、对未标注文本进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabel_news_words_jieba = text2words(train_unlabel_bunch.news_content,postdict=postdict)   # jieba 分词 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unlabel_news_words_jieba)\n",
    "# unlabel_news_words_jieba[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、训练集的标签匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['news_content', 'news_id', 'news_pic_list']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_bunch.news_id)):\n",
    "    if train_bunch.news_id[i]!=train_label.news_pic_id[i]:\n",
    "        print(train_bunch.news_id[i],'is not equal with',train_label.news_pic_id[i])\n",
    "        break\n",
    "    else:\n",
    "        pass\n",
    "#         print('ID is correct!')\n",
    "dir(train_bunch)\n",
    "dir(train_label)\n",
    "print(dir(validate_bunch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5、将分词结果保存以方便后续使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1、将训练集与验证集进行Bunch化 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['news_pic_id', 'news_pic_label', 'news_pic_pic', 'news_pic_text']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.base import Bunch\n",
    "\n",
    "# 1、将训练集的分词结果保存为本地的一个 Bunch 对象\n",
    "TRAIN_BUNCH = Bunch(news_id=[],news_words_jieba=[],news_pic_list=[])\n",
    "TRAIN_BUNCH.news_id = train_bunch.news_id\n",
    "TRAIN_BUNCH.news_words_jieba = train_content_words_jieba\n",
    "TRAIN_BUNCH.news_pic_list = train_bunch.news_pic_list\n",
    "\n",
    "# 2、将训练集新闻和图片对应的标签保存为本地的一个 Bunch 对象\n",
    "TRAIN_LABEL_BUNCH = Bunch(news_pic_id=[],news_pic_label=[],news_pic_pic=[],news_pic_text=[])\n",
    "TRAIN_LABEL_BUNCH.news_pic_id = train_label.news_pic_id\n",
    "TRAIN_LABEL_BUNCH.news_pic_label = train_label.news_pic_label\n",
    "TRAIN_LABEL_BUNCH.news_pic_pic = train_label.news_pic_pic\n",
    "TRAIN_LABEL_BUNCH.news_pic_text = train_label.news_pic_text\n",
    "\n",
    "# 3、将未标注数据集的分词结果保存为本地的一个 Bunch 对象\n",
    "UNLABEL_NEWS_BUNCH = Bunch(news_id=[],news_words_jieba=[],news_pic_list=[])\n",
    "UNLABEL_NEWS_BUNCH.news_id = train_unlabel_bunch.news_id\n",
    "UNLABEL_NEWS_BUNCH.news_words_jieba = unlabel_news_words_jieba\n",
    "UNLABEL_NEWS_BUNCH.news_pic_list = train_unlabel_bunch.news_pic_list\n",
    "\n",
    "# 4、将验证集（contain id, content and pictures list）保存为一个本地的 Bunch 对象\n",
    "VALIDATE_BUNCH = Bunch(news_id=[],news_words_jieba=[],news_pic_list=[])\n",
    "VALIDATE_BUNCH.news_id = validate_bunch.news_id\n",
    "VALIDATE_BUNCH.news_words_jieba = validate_content_words_jieba\n",
    "VALIDATE_BUNCH.news_pic_list = validate_bunch.news_pic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_pic_list</th>\n",
       "      <th>news_words_jieba</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>D0048481</th>\n",
       "      <td>P0351731.JPEG;P0351732.JPEG;P0351733.JPEG;P035...</td>\n",
       "      <td>历代 名家 书法 对联 过年 写 春联 再也 不求人</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D0048482</th>\n",
       "      <td>P0351764.JPEG\\n</td>\n",
       "      <td>古代 街头 表演 人多 拥挤 怎么办 梅花拳 竟想 出 此法 武术 在 广大 民间 具有 深...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D0048483</th>\n",
       "      <td>NULL\\n</td>\n",
       "      <td>风情万种 你 要 哪种 风情万种 你 要 哪种</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D0048484</th>\n",
       "      <td>P0351765.JPEG;P0351766.JPEG;P0351767.JPEG;P035...</td>\n",
       "      <td>今天 请 把 这首 从 此刻 起 我要 读 3 遍 今天 把 这首 从 此刻 起 我要 强烈...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D0048485</th>\n",
       "      <td>P0351769.JPEG\\n</td>\n",
       "      <td>重磅 江西 这家 省直 单位 正式 挂牌 成立 2 月 8 日 上午 省委 军民 融合 发展...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D0048486</th>\n",
       "      <td>NULL\\n</td>\n",
       "      <td>2018 华南 师范大学 网络 教育 音乐学 师范 本科 介绍 华南 师范大学 网络 教育 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D0048487</th>\n",
       "      <td>P0351770.JPEG;P0351771.JPEG;P0351772.JPEG;P035...</td>\n",
       "      <td>寒假 集结 号 尚翔 篮球 集训营 火热 招生 中 篮球 不仅 是 一项 体育运动 它 是...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D0048488</th>\n",
       "      <td>P0351780.JPEG;P0351781.JPEG;P0351782.JPEG;P035...</td>\n",
       "      <td>宝 夜读 森林 里 的 陌生 来客 生命 教育 欢迎 大家 进店 选购 乐 乐趣 图书 销...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D0048489</th>\n",
       "      <td>P0351789.JPEG;P0351790.JPEG;P0351791.JPEG;P035...</td>\n",
       "      <td>学会 这 几招 保证 节前 综合症 不再 折磨 你 2018 年 的 春节 就 这样 猝不及...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D0048490</th>\n",
       "      <td>P0351796.JPEG;P0351797.JPEG;P0351798.JPEG;P035...</td>\n",
       "      <td>惊呆 昨天 有 100 位 HR 当众 表白 情人节 中奖 名单 公布 是 时候 揭晓 小 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              news_pic_list  \\\n",
       "news_id                                                       \n",
       "D0048481  P0351731.JPEG;P0351732.JPEG;P0351733.JPEG;P035...   \n",
       "D0048482                                    P0351764.JPEG\\n   \n",
       "D0048483                                             NULL\\n   \n",
       "D0048484  P0351765.JPEG;P0351766.JPEG;P0351767.JPEG;P035...   \n",
       "D0048485                                    P0351769.JPEG\\n   \n",
       "D0048486                                             NULL\\n   \n",
       "D0048487  P0351770.JPEG;P0351771.JPEG;P0351772.JPEG;P035...   \n",
       "D0048488  P0351780.JPEG;P0351781.JPEG;P0351782.JPEG;P035...   \n",
       "D0048489  P0351789.JPEG;P0351790.JPEG;P0351791.JPEG;P035...   \n",
       "D0048490  P0351796.JPEG;P0351797.JPEG;P0351798.JPEG;P035...   \n",
       "\n",
       "                                           news_words_jieba  \n",
       "news_id                                                      \n",
       "D0048481                        历代 名家 书法 对联 过年 写 春联 再也 不求人   \n",
       "D0048482  古代 街头 表演 人多 拥挤 怎么办 梅花拳 竟想 出 此法 武术 在 广大 民间 具有 深...  \n",
       "D0048483                           风情万种 你 要 哪种 风情万种 你 要 哪种   \n",
       "D0048484  今天 请 把 这首 从 此刻 起 我要 读 3 遍 今天 把 这首 从 此刻 起 我要 强烈...  \n",
       "D0048485  重磅 江西 这家 省直 单位 正式 挂牌 成立 2 月 8 日 上午 省委 军民 融合 发展...  \n",
       "D0048486  2018 华南 师范大学 网络 教育 音乐学 师范 本科 介绍 华南 师范大学 网络 教育 ...  \n",
       "D0048487   寒假 集结 号 尚翔 篮球 集训营 火热 招生 中 篮球 不仅 是 一项 体育运动 它 是...  \n",
       "D0048488   宝 夜读 森林 里 的 陌生 来客 生命 教育 欢迎 大家 进店 选购 乐 乐趣 图书 销...  \n",
       "D0048489  学会 这 几招 保证 节前 综合症 不再 折磨 你 2018 年 的 春节 就 这样 猝不及...  \n",
       "D0048490  惊呆 昨天 有 100 位 HR 当众 表白 情人节 中奖 名单 公布 是 时候 揭晓 小 ...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.DataFrame.from_dict(VALIDATE_BUNCH).set_index('news_id')\n",
    "train_df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2、将训练集和验证集保存至本地指定的目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from instrument import save_bunch\n",
    "\n",
    "train_bunch_path = './data_bunch/non_stop_words_train_bunch.dat'\n",
    "save_bunch(train_bunch_path,TRAIN_BUNCH)\n",
    "\n",
    "train_label_bunch_path = './data_bunch/train_label_bunch.dat'\n",
    "save_bunch(train_label_bunch_path,TRAIN_LABEL_BUNCH)\n",
    "\n",
    "unlabel_news_bunch_path = './data_bunch/non_stop_words_unlabel_news_bunch.dat'\n",
    "save_bunch(unlabel_news_bunch_path,UNLABEL_NEWS_BUNCH)\n",
    "\n",
    "validate_bunch_path = './data_bunch/non_stop_words_validate_bunch.dat'\n",
    "save_bunch(validate_bunch_path,VALIDATE_BUNCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6、分词后结果处理（专业名词替换）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyltp import Segmentor\n",
    "model_path = 'E:/Desktop/ZhuFei/Competition/NLP/ltp_data_v3.4.0/cws.model'   # Ltp 3.4 分词模型库\n",
    "segmentor = Segmentor()\n",
    "segmentor.load(model_path)\n",
    "words = segmentor.segment('在包含问题的所有解的解空间树中，按照深度优先搜索的策略，从根节点出发深度搜索解空间树。')\n",
    "seg_sent = ' '.join(words)\n",
    "print(seg_sent)\n",
    "postdict = {'解 空间':'解空间','深度 优先':'深度优先'}\n",
    "for key in postdict:\n",
    "    seg_sent = seg_sent.replace(key, postdict[key])\n",
    "print(seg_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建词向量空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将储存训练集和验证集分词的Bunch对象读入内存，用以进行模型训练 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read bunch object\n",
    "from instrument import read_bunch\n",
    "\n",
    "train_bunch_path = './data_bunch/non_stop_words_train_bunch.dat'\n",
    "train_label_bunch_path = './data_bunch/train_label_bunch.dat'\n",
    "validate_bunch_path = './data_bunch/non_stop_words_validate_bunch.dat'\n",
    "\n",
    "train_bunch = read_bunch(train_bunch_path)\n",
    "train_label_bunch = read_bunch(train_label_bunch_path)\n",
    "validate_bunch = read_bunch(validate_bunch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.base import Bunch\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、构建训练集的 TF-IDF 词向量空间对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_bunch.news_words_jieba[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train = Bunch(Id=train_bunch.news_id, Label=train_label_bunch.news_pic_label, tdm=[], vocabulary={})\n",
    "train_vectorizer = TfidfVectorizer(max_features=350000)\n",
    "tfidf_train.tdm = train_vectorizer.fit_transform(train_bunch.news_words_jieba)        # jieba 分词结果\n",
    "tfidf_train.vocabulary = train_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_train.vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、构建验证集的 TF-IDF 词向量空间对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_validate = Bunch(Id=validate_bunch.news_id, tdm=[], vocabulary={})\n",
    "tfidf_validate.vocabulary = tfidf_train.vocabulary\n",
    "validate_vectorizer = TfidfVectorizer(vocabulary=tfidf_train.vocabulary)\n",
    "tfidf_validate.tdm = validate_vectorizer.fit_transform(validate_bunch.news_words_jieba)        # jieba 分词结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_train.vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将训练数据划分为训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(tfidf_train.tdm,\n",
    "                                                    tfidf_train.Label,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of classifying training data with Naive Bayes is : 0.6613036303630363\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.79      0.73      7192\n",
      "          1       0.47      0.03      0.06      2357\n",
      "          2       0.65      0.77      0.71      4995\n",
      "\n",
      "avg / total       0.63      0.66      0.61     14544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Multinomial Naive Bayes Classifier\n",
    "def classifier_naive_bayes(x_data, y_labels):\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    model = MultinomialNB(alpha=0.1453)\n",
    "    model.fit(x_data, y_labels)\n",
    "    return model\n",
    "model_naive_bayes = classifier_naive_bayes(x_train, y_train)\n",
    "print('The accuracy of classifying training data with Naive Bayes is :',\n",
    "      model_naive_bayes.score(x_test, y_test))\n",
    "print(classification_report(y_test,model_naive_bayes.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、验证集预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_naive_bayes = classifier_naive_bayes(tfidf_train.tdm,tfidf_train.Label)\n",
    "predict_naive_bayes = model_naive_bayes.predict(tfidf_validate.tdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. store the result of predict to local, and ust it to submittion\n",
    "bayes_text = []\n",
    "for i in range(len(validate_bunch.news_id)):\n",
    "    bayes_text.append('NULL')\n",
    "\n",
    "label_predict = predict_naive_bayes\n",
    "bayes_result = []\n",
    "for i in range(len(validate_bunch.news_id)):\n",
    "    bayes_result.append(validate_bunch.news_id[i]+'\\t'+label_predict[i]+'\\t'+bayes_text[i]+'\\t'+bayes_text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from instrument import save_text\n",
    "\n",
    "save_path = './submittion/result_bayes.txt'\n",
    "save_text(save_path, bayes_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入数据\n",
    "from instrument import read_bunch\n",
    "from sklearn.datasets.base import Bunch\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# load data from local\n",
    "train_bunch_path = './data_bunch/train_bunch.dat'\n",
    "validate_bunch_path = './data_bunch/validate_bunch.dat'\n",
    "train_bunch = read_bunch(train_bunch_path)\n",
    "validate_bunch = read_bunch(validate_bunch_path)\n",
    "\n",
    "# 创建词向量空间\n",
    "stop_words_list = None\n",
    "max_df = 0.7\n",
    "\n",
    "# create TF-IDF words vector space with train data\n",
    "tfidf_train = Bunch(Id=train_bunch.news_id, Label=train_bunch.news_pic_label, tdm=[], vocabulary={})\n",
    "train_vectorizer = TfidfVectorizer(stop_words=stop_words_list, sublinear_tf=True, max_df=max_df)\n",
    "tfidf_train.tdm = train_vectorizer.fit_transform(train_bunch.news_words_jieba)                # jieba 分词结果或\n",
    "tfidf_train.vocabulary = train_vectorizer.vocabulary_\n",
    "\n",
    "# create TF-IDF words vector space with validate data\n",
    "tfidf_validate = Bunch(Id=validate_bunch.news_id, tdm=[], vocabulary={})\n",
    "tfidf_validate.vocabulary = tfidf_train.vocabulary\n",
    "validate_vectorizer = TfidfVectorizer(stop_words=stop_words_list, sublinear_tf=True, max_df=max_df,\n",
    "                                      vocabulary=tfidf_train.vocabulary)\n",
    "tfidf_validate.tdm = validate_vectorizer.fit_transform(validate_bunch.news_words_jieba)        # jieba 分词结果\n",
    "\n",
    "# 将数据分为训练集与测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(tfidf_train.tdm,\n",
    "                                                    tfidf_train.Label,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=33)\n",
    "\n",
    "# 构建模型\n",
    "from sklearn.metrics import classification_report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
