{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.009724473257698542\n",
      "0.019448946515397084\n",
      "0.029173419773095625\n",
      "0.03889789303079417\n",
      "0.75\n",
      "0.78\n",
      "0.81\n",
      "0.84\n",
      "0.87\n",
      "0.9\n",
      "0.916\n",
      "0.932\n",
      "0.948\n",
      "0.964\n",
      "0.98\n",
      "0.996\n",
      "0.001\n",
      "0.003\n",
      "0.005\n",
      "0.007\n",
      "0.009\n"
     ]
    }
   ],
   "source": [
    "for min_df in [i/1234 for i in range(0,50,12)]:                     # 控制 min_df\n",
    "    print(min_df)\n",
    "for max_df in [i/100 for i in range(75,90,3)]:               # 控制 max_df\n",
    "    print(max_df)\n",
    "for feature_percent in [i/1000 for i in range(900,999,16)]:  # 控制参加模型训练的特征的比例\n",
    "    print(feature_percent)\n",
    "for alpha in [i/1000 for i in range(1,10,2)]:          # 控制朴素贝叶斯模型的学习速率 alpha\n",
    "    print(alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 云移动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of raw_data: <class 'list'>\n",
      "The length of raw_data: 100001\n"
     ]
    }
   ],
   "source": [
    "# coding = utf-8\n",
    "\"\"\"将训练集进行分词后以Bunch的形式进行存储\"\"\"\n",
    "import csv\n",
    "# load discuss data\n",
    "data_path = 'E:/Desktop/ZhuFei/CNN/TensorFlow/云移动/subject_1/train_first.csv'\n",
    "open_discuss_data = open(data_path, encoding='utf-8')\n",
    "discuss_data = csv.reader(open_discuss_data)\n",
    "\n",
    "# convert the discuss_data as a list file\n",
    "raw_data = [row for row in discuss_data]\n",
    "print(\"The type of raw_data:\", type(raw_data))\n",
    "print(\"The length of raw_data:\", len(raw_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract 'Id', 'Discuss' and 'Score'\n",
    "discuss_id = []\n",
    "discuss_content = []\n",
    "discuss_score = []\n",
    "\n",
    "for index in range(1, len(raw_data)):\n",
    "    discuss_id.append(raw_data[index][0])\n",
    "    discuss_content.append(raw_data[index][1])\n",
    "    discuss_score.append(raw_data[index][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# omit html\n",
    "from lxml import html\n",
    "discuss_content_text = []\n",
    "for content in discuss_content:\n",
    "    page = html.document_fromstring(content)\n",
    "    discuss_content_text.append(page.text_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\kara\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.175 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# divide sentence to word\n",
    "import jieba\n",
    "discuss_content_word = []\n",
    "for sentence in discuss_content_text:\n",
    "    content = sentence.replace('\\r\\n', '')    # 删除换行符\n",
    "    content = content.replace(' ', '')        # 删除空行、多余的空格\n",
    "    content_word = jieba.cut(content)\n",
    "    discuss_content_word.append(' '.join(content_word))  # 每条评论的词组通过空格隔开，存储为Excel形式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discuss_content_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store data\n",
    "# import pickle\n",
    "# store_data_path = 'E:/Desktop/ZhuFei/CNN/TensorFlow/云移动/subject_1/train_set.dat'\n",
    "# with open(store_data_path, 'wb') as file_obj:\n",
    "#     pickle.dump(bunch, file_obj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建IF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# 停用词表\n",
    "stopword_path = 'E:/Desktop/ZhuFei/CNN/TensorFlow/云移动/subject_1/stop_words.txt'\n",
    "stopwdlst = None\n",
    "readfile\n",
    "readfile(stopword_path).splitlines()\n",
    "# 构建训练集的tf-idf词向量空间对象\n",
    "tfidf_train = Bunch(Id=train_bunch.review_id, Score=train_bunch.review_score, tdm=[], vocabulary={})\n",
    "train_vectorizer = TfidfVectorizer(stop_words=stopwdlst, sublinear_tf=True, max_df=0.5)\n",
    "tfidf_train.tdm = train_vectorizer.fit_transform(train_bunch.review_content)\n",
    "tfidf_train.vocabulary = train_vectorizer.vocabulary_\n",
    "# 构建测试集的tf-idf词向量空间对象\n",
    "tfidf_test = Bunch(Id=test_bunch.review_id, tdm=[], vocabulary={})\n",
    "tfidf_test.vocabulary = tfidf_train.vocabulary\n",
    "test_vectorizer = TfidfVectorizer(stop_words=stopwdlst, sublinear_tf=True,\n",
    "                                  max_df=0.5, vocabulary=tfidf_train.vocabulary)\n",
    "tfidf_test.tdm = test_vectorizer.fit_transform(test_bunch.review_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
